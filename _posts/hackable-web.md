---
layout: post
title: The Hacker's Web
desc: The Web is dependent on services, and that stifles innovation. We can fix that by distributing authority to users (via P2P publishing).
---

<style>
hr { margin: 2rem 0; }
.post__content { padding-top: 0; }
h2 { margin-top: 4rem !important; }
</style>

---

**TL;DR.** The Web is dependent on services, and that stifles innovation. We can fix that by distributing authority to users with P2P publishing.

---

This post continues my series on the [Beaker Project](#todo). Previous articles:

 - [What is the P2P Web?](#todo)
 - [Achieving Scale with Decentralization](#todo)


## Motivation
<br>

The Web's applications are too centralized. Patches come from the top down, and I want them to come from the *bottom up*. Everyone should have the power to change the Web for the better.

Web apps should be user-hackable. We should be able to

1. Modify sites, and share the modifications with other users.
2. Integrate applications in ways not predicted by their developers.
3. Switch apps without leaving behind our content or network.

I want the Web to be socially constructed. I want the software to be free and open source. And, I want a massively-multiplayer programming environment.


## A return to thick clients
<br>

We need to move away from the thin-client / thick-service model.

Services lock portions of functionality away from users. They are not designed to be modified by users; it's just too complicated to have untrusted users pushing custom software to the data-center.

So, we need to take business logic out of services, and put it into the users' devices, where they can make changes safely.

User devices are extremely capable. 







## What would a "Hackable Web" look like?
<br>


3. Applications would be able to communicate around shared datasets and interfaces.
2. Users would be able to configure new applications and services for common tasks.

The first improvement is, users wont’t need to keep using the application they first create with. The content is stored on sites in local folders, and could be accessed by any application they choose. Walled garden? Gone. The second improvement is on the consumer side: just as authors don’t need to stick with the creating app, consumers can pick their own application as well. As all content is independently published, it can be fetched and consumed by any application.





## Result: Innovation is stifled

Users can't easily switch sites (network effect) and they can't easily modify the sites (service authority). All decisions now have to be made by non-users, from the top down.

At the core of this is a question of "Citizenry." Users and their content need to have independent URLs. We need to promote users to first-class citizens on the Web.

---

## n. 1 website = 1 identity

All users should have many identities, and thus many personal websites. They should create the sites freely as with service accounts, and use them to publish their online interactions and content.

---

## 5. A distributed backend

Dat and IPFS are 

---

## Orchestrated chaos


## A digital society


On Twitter, this may not matter much, because they're just tweets. In services like YouTube, where creators make their livelihoods, the issue of control is more important and can be contentious, but the trade still works. [https://kotaku.com/why-youtubers-are-freaking-out-about-money-and-censorsh-1786032317] The actual damage of service-dependent publishing is a secondary effect: it stifles innovation. Content is posted with applications; applications are tied to services; and the services use closed databases. The services get to choose the rules for their applications, and they don't always make it easy to modify or extend the experience. Users are disempowered to control their content, and programmers are therefore disempowered to give users new software.

Fdederated services don't fix the fundamental problem of this relationship: users continue to depend on services to publish, and the services still dictate the terms of use. The only first-class citizen of the Web is the "Website." Therefore, the only meaningful solution is to promote users to first-class citizens.

Does this mean that users should each have their own Website? No, better! It means that each user should have *many* Websites. They should create them freely, and plentifully, and cheaply, and use them to do all of the publishing and interacting online that services normally do. With the P2P Web, not only is this possible, it's the more convenient option.

The P2P Web protocols -- of which there are two Beaker which uses, Dat & IPFS [2] -- are peer-to-peer file networks. Their sites are mutable file-bundles, which are synced in a mesh similar to BitTorrent. The cost is nil to allocate and deploy these sites, and they can be generated by Web applications using Web Platform APIs. As a result, any computer can host many sites, which applications can read/write to, and there is no need to use a service to do the publishing.

This should conjure some very Web 1.0 imagery, but the technology is much more interesting. It has never been quite this easy to allocate new sites. Any computer in the P2PW, at any time, can generate an new domain by minting a keypair. This is so cheap, that we can allow Web applications to do it by API. Publishing to these sites is a write to the local FS. Therefore, applications can publish content by writing files into other sites. Many of these sites will probably contained structured data -- .json files -- to be consumed by other application-sites which contain HTML and JS. As a convenient side-effect, P2PW sites can accept writes whether the Internet is available or not, and so they are a strong offline-first technology (comparible in this regard to Pouch/CouchDB).

It's helpful that the P2P Web also has much better caching properties than the traditional Web, due to active syncronization. There's no need to check for cache invalidation over the network on every read. Instead, the browser engages in a background swarm, which is effectively a long-poll that costs a few UDP packets per hour. The browser, per its users' instructions, will actively syncronize a set of sites, which in turn populates the users' applications.

What about uptime? The sites are controlled by private keys stored on the user's machine and identified by public keys, which means there is local sovereignty, but there isn't a need to host from a specific device; a site can be rehosted by any machine. So, creators can take advantage of public peer services, to provide uptime, but this is a background concern. The cloud is a dumb and fungible resource, as it really should be.

With all this said, we can now address our problem of innovation. Each user now controls their content; what's the effect on software?

The first improvement is, users wont't need to keep using the application they first create with. The content is stored on sites in local folders, and could be accessed by any application they choose. Walled garden? Gone. The second improvement is on the consumer side: just as authors don't need to stick with the creating app, consumers can pick their own application as well. As all content is independently published, it can be fetched and consumed by any application.


A natural (and all-important) final question to ask is, how will this scale?
The peer-to-peer arrangement is 




 ^1 The exceptions are A) browser extensions, which are brittle and limited in scope, and B) third-party clients, which perform poorly and are subject to the service's decisions. The organization that controls the community's database dictates the experience.

 ^2 Why Beaker has two protocols, and how the differ, will be the subject of another post. They are very similar. You can read about Dat at https://beakerbrowser.com/docs/dat/intro.html, and about IPFS at https://ipfs.io/